# 13. Недообучение и переобучение. Основные подходы к выявлению переобучения. Признак переобученности линейной модели. Мультиколлинеарность признаков, ее связь с возможностью переобучения модели. [[⇧]](../questions-list.md)

## Недообучение

**Недообучение** — ситуация, когда алгоритм плохо описывает и обучающую выборку, и новые данные.

`Возможная причина`: используемая модель слишком **простая**.

`Пример`: используется линейная модель, в то время, когда на самом деле имеет место нелинейная зависимость.

`Решение`: усложнение модели.

## Переобучение

**Переобучение** — ситуация, когда алгоритм хорошо описывает обучающую выборку, но плохо описывает новые данные.

`Возможная причина`: используемая модель слишком **сложная**.

`Пример`: в используемой модели слишком большие веса признаков, модель не извлекла общую закономерность, а просто подстроилась под данные.

`Решение`: в линейных моделях — регуляризация.

### Основные подходы к выявлению переобучения

- **Использование отложенной (тестовой) выборки** — часть из имеющихся данных не участвует в обучении; на этих данных проверяется обученный алгоритм.
- **Кросс-валидация** — усложненный метод отложенной выборки.
- Использование **меры сложности моделей**.

### Признаки переобученности линейной модели

- Высокая точность на обучающей выборке, но низкая точность на тестовой выборке.
- Очень большие или очень маленькие значения коэффициентов модели.
- Высокая дисперсия остатков (разница между предсказанными и фактическими значениями).
- Наличие шума в данных может привести к переобучению модели, если она пытается подстроиться под этот шум.

## Мультиколлинеарность признаков, ее связь с возможностью переобучения модели

**Мультиколлинеарность** — ситуация, когда признаки в выборке являются **линейно зависимыми**.

Определение: существуют коэффициенты $a_1, a_2, \ldots, a_d$ , не равные одновременно нулю, такие, что для любого объекта $x_i  \in X$ выполняется:

$$
a_1 \cdot x_i^1 + a_2 \cdot x_i^2 + \ldots + a_d \cdot x_i^d =0
$$

### Проблема мультиколлинеарности

Матрица $X^T \cdot X$ вырождена — аналитическое решение задачи минимизации невозможно:

$$w_{\ast}=(X^T \cdot X)^{-1} \cdot X^T \cdot y$$

$(X^T \cdot X)^{-1}$ – обратной матрицы не существует.

> Применение численных методов оптимизации может привести к **переобучению**.

В случае мультиколлинеарности признаков решение — **бесконечное** множество алгоритмов, среди этих алгоритмов:

- Многие имеют большие веса.
- Не все имеют хорошую обобщающую способность.
