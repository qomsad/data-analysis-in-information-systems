# 12. Вычислительные сложности градиентного спуска. Алгоритм стохастического градиентного спуска (SGD). Особенности сходимости градиентного спуска. Преимущества метода SGD. [[⇧]](../questions-list.md)

## Вычислительные сложности градиентного спуска

Выражение для $j$-й компоненты градиента содержит суммирование по **всем** объектам обучающей выборки:

$$
\frac{\partial Q}{\partial w_j} = \frac{2}{n} ∑_{i=1}^{n} (\langle w_j, x_i^j\rangle - y_i) \cdot x_i, \quad j = 1,2,3,4, \ldots, d
$$

В этой сумме $i$-ое слагаемое показывает, как нужно изменить вес $w^i$ , чтобы качество алгоритма увеличилось для $i$-го объекта выборки. При этом вся сумма определяет, как нужно изменить $w_j$, чтобы повысить качество для всех объектов выборки.

> В случае большой выборки даже одна итерация метода требует больших вычислительных ресурсов.

## Алгоритм стохастического градиентного спуска (SGD)

Идея **стохастического градиентного спуска (SGD)** — вычислять градиент функции качества только на **одном** случайно выбранном объекте обучающей выборки.

> Это позволяет существенно снизить объем вычислений на каждой итерации.

**Алгоритм** стохастического градиентного спуска:

1. **Инициализация**, задание начальных весов:

   - $w^0=0$ – нулевой вектор.

2. **Цикл** по $t=1, 2, 3,\ldots,$ _(номер итерации)_:

   - Выбор случайного объекта $x \in X$.

   - Построение очередного приближения вектора весов $w^t$ по приближению $w^{t-1}$, найденному на предыдущей итерации:
     $$w^t=w^{t-1}-\eta_t \cdot \nabla Q(w^{t-1},\lbrace x \rbrace)$$

3. **Условие останова** может задаваться по‑разному:
   $$||w^t-w^{t-1}|| < \varepsilon \quad (1)$$

   $$||\nabla Q(w^{t})|| < \varepsilon \quad (2)$$

   $\varepsilon$ – заранее заданная малая величина.

> В обычном градиентном спуске на каждом шаге уменьшается суммарная ошибка на всех элементах обучающей выборки — функция монотонно убывает.

Стохастический метод работает так, чтобы максимально уменьшить ошибку для одного **случайного** объекта — на каждой итерации полная ошибка может как увеличиваться, так и уменьшаться.

### Преимущества стохастического градиентного спуска

- Каждый шаг выполняется существенно быстрее шага обычного метода.
- Не требуется постоянно хранить всю обучающую выборку в памяти.
- Подходит для онлайн-обучения (алгоритм на каждом шаге получает только один объект и должен использовать его для коррекции модели).
