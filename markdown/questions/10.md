# 10. Возможность аналитического решения задачи обучения регрессии; недостатки этого подхода. Оптимизационный подход. Градиент функции, его свойства. Алгоритм градиентного спуска (общий случай). [[⇧]](../questions-list.md)

## Аналитическое решение

Задача регрессии имеет **аналитическое** решение:

$$w_{\ast}=(X^T \cdot X)^{-1} \cdot X^T \cdot y$$

$X$ – выборка, представленная матрицей «объекты–признаки».

$y$ – вектор ответов.

$w_{\ast}$ – вектор весов (коэффициентов), который мы хотим определить.

> 0. Пусть отклонение равно $0$, тогда модель будет выдавать правильные ответы для каждого объекта выборки:
>    $$X \cdot w - y =0$$
>
> 1. Тогда:
>    $$X \cdot w = y$$
> 2. Домножим каждую часть уравнения на транспонированную матрицу $X^T$:
>    $$X^T \cdot X \cdot w = X^T \cdot y$$
> 3. Так как результат $X^T \cdot X$ всегда квадратная матрица, найдем её обратную и домножим обе части уравнения на $(X^T \cdot X)^{-1}$:
>    $$(X^T \cdot X)^{-1} \cdot X^T \cdot X \cdot w= (X^T \cdot X)^{-1} \cdot X^T \cdot y$$
> 4. Заметим $(X^T \cdot X)^{-1} \cdot X^T \cdot X$ – единичная матрица, значит:
>    $$w= (X^T \cdot X)^{-1} \cdot X^T \cdot y$$
> 5. Задача имеет решение, если **изначальная выборка не содержит линейно зависимых признаков** (в этом случаем матрица $X^T \cdot X$ вырождена, а значит невозможно найти обратную к ней).

**Сложности этого подхода**:

- Необходимость вычисления обратной матрицы порядка $d$, требует выполнения порядка $d^3$ операций.
- Численные методы обращения матриц не работают в случае плохо обусловленных матриц (небольшое изменение входных данных сильно влияет на ответ).

## Оптимизационный подход

Можно показать, что функционал ошибки — **гладкая выпуклая функция**:

- **Выпуклость** гарантирует существование и единственность **точки минимума**.
- **Гладкость** гарантирует существование **вектора градиента** в любой точке.

> Значит можно использовать **градиентные** методы оптимизации.

### Градиент функции и его свойства

**Градиент** — **вектор** частных производных[^1] функции:
$$\nabla f(t_1, t_2, \ldots, t_k)=\Big(\frac{\partial f}{ \partial t_1}, \frac{\partial f}{\partial t_2}, \ldots , \frac{\partial f}{\partial t_k}\Big)$$

Градиент функции, вычисленный в заданной точке $t_j = (t_{j1}, t_{j2}, \ldots, t_{jk})$, показывает направление наискорейшего **возрастания** функции в данной точке.

> 0. Пусть дана функция:
>    $$f(x,y) = 2x^3y^2 - 16y^2 - 216x - 5$$
> 1. Вычислим частную производную по $x$:
>    $$\frac{\partial f}{ \partial x} = (2x^3y^2 - 16y^2 - 216x - 5)_x^{'} = 2y^2 \cdot (x^3)_x^{'} - 216 \cdot (x)_x^{'}=$$
>    $$
>    = 2y^2 \cdot 3x^2 - 216x \cdot 1 = 6x^2y^2  - 216
>    $$
> 2. Вычислим частную производную по $y$:
>    $$\frac{\partial f}{ \partial y} = (2x^3y^2 - 16y^2 - 216x - 5)_y^{'} = 2x^3 \cdot (y^2)_y^{'} - 16 \cdot (y^2)_y^{'} = 4x^3y - 32y$$
> 3. Градиент функции $f$:
>    $$
>    \nabla f(x,y) = \begin{pmatrix} 6x^2y^2  - 216 \\
>    4x^3y - 32y \end{pmatrix}
>    $$
> 4. Вычислим градиент в точке $p = (2, 3)$:
>    $$
>    \nabla f(2, 3) = \begin{pmatrix} 6 \cdot 2^2 \cdot 3^2  - 216 \\
>    4 \cdot 2^3 \cdot 3 - 32 \cdot 3 \end{pmatrix} = (0, 0)
>    $$
> 5. Так как $p = (2, 3)$, точка экстремума функции $f$, то:
>
>    $$
>    ||\nabla f(2, 3)|| = \sqrt{0^2 + 0^2} = 0
>    $$

Если точка $t_j$ — **точка экстремума функции** и в ней существуют частные производные, то:
$$||\nabla f(t_j)||=0$$

**Антиградиент** — вектор, противоположный градиенту:
$$- \nabla f(t_j)$$
Он показывает направление наискорейшего **убывания** функции в данной точке.

> Значит антиградиент можно использовать для организации поиска **минимума** функции.

### Алгоритм градиентного спуска

**Идея алгоритма**:

1. **Инициализация** — старт из случайной точки (случайные значения весов).
2. **Цикл** — шаг в направлении антиградиента.
3. **Условие останова** — достижение минимума.

**Алгоритм (общий случай)**:

1. **Инициализация**, задание начальных весов:

   - Самый простой подход $w^0=0$ – нулевой вектор.

2. **Цикл** по $t=1, 2, 3,\ldots,$ _(номер итерации)_:

   - Построение очередного приближения вектора весов $w^t$ по приближению $w^{t-1}$, найденному на предыдущей итерации:
     $$w^t=w^{t-1}-\eta_t \cdot \nabla Q(w^{t-1},X)$$

     $\eta_t$ – числовой коэффициент (шаг).

     $\nabla Q(w^{t-1},X)$ – градиент функции $Q$, вычисленный в точке $w^{t-1}$.

3. **Условие останова** может задаваться по‑разному:
   $$||w^t-w^{t-1}|| < \varepsilon \quad (1)$$

   $$||\nabla Q(w^{t})|| < \varepsilon \quad (2)$$

   $\varepsilon$ – заранее заданная малая величина

[^1]: Частная производная — производная вычисленная по одной переменной (остальные принимаются за константны).
