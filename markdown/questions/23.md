# 23. Критерий ошибки в решающих деревьях. Критерии информативности в задачах регрессии и классификации. [[⇧]](../questions-list.md)

## Критерий ошибки

$$ Q (X_m ,j,t) = \frac{|X_l |}{|X_m |} \cdot H (X_l ) + \frac{|X_l |}{|X_m |} \cdot H (X_r )$$

$X_m$ – множество объектов в вершине $m$, подлежащих разбиению.

$X_l$ – множество объектов в левой дочерней вершине, полученной после разбиения вершины $m$.

$\frac{|X_l |}{|X_m |}$ – доля подвыборки $X_l$ в выборке $X_m$.

$H(X_l )$ – разброс ответов в левом поддереве.

$X_r$ – множество объектов в правой дочерней вершине, полученной после разбиения вершины $m$.

$\frac{|X_r |}{|X_m |}$ – доля подвыборки $X_r$ в выборке $X_m$.

$H(X_r)$ – разброс ответов в левом поддереве.

$H(X)$ – критерий информативности, характеризует разброс ответов в $X$.

## Критерий информативности в задаче регрессии

В качестве меры **разброса** ответов может быть выбрана **дисперсия** ответов:

$$H (X) = \frac{1}{|X|} \cdot ∑_{x_i \in X}(y_i - \overline{y}(X))^2$$

$$\overline{y}(X) = \frac{1}{|X|} \cdot ∑_{x_i \in X} y_i$$

## Критерий информативности в задачах классификации

### Энтропия

Вероятности классов – долии соответствующих ответов в вершине:

$$
p_k = \frac{1}{|X|} \cdot ∑_{x_i in X}[y_i = k]
$$

- Нулевая энтропия $p_k = 0$ если в вершине только один класс.
- Максимальная энтропия $p_k = 1$ если в вершине поровну объектов всех классов.

### Энтропийный критерий

$$H(X) = -∑_{k=1}^{K}p_k \ln p_k $$

- **Свойства**:
  - $H(X) \ge 0$ для любой выборки $X$.
  - $H(X)$ достигает минимума только в том случае, когда все объекты $X$ относятся к одному классу.

### Критерий Джини

$$H(X) =∑_{k=1}^{K}p_k \cdot (1- p_k)$$

`Пример:` Строится решающее дерево для задачи классификации на три класса по двум входным признакам. Дана обучающая выборка $X$ и вектор правильных ответов $y$.

$$
X = \begin{pmatrix} 10 & 4 \\
2 & 2 \\
9 & 9 \\
2 & 1 \\
8 & 2 \\
3 &4 \\
9 & 5 \end{pmatrix}, \quad y= \begin{pmatrix} 2 \\
1 \\
2 \\
1 \\
3 \\
2 \\
3 \end{pmatrix}
$$

Используя критерий Джини, определить какой из предикатов: $[1$-й признак $< 5]$ или $[2$-й признак $< 3]$ лучше.

Построим правое и левое поддерево для критерия $[1$-й признак $< 5]$:

$$
\begin{matrix}
X_l & X_r \\
x=(2,2)\ \ y=1 & x=(10,4)\ \ y=2 \\
x=(2,1)\ \ y=1 & x=(9,9)\ \ y=2 \\
x=(3,4)\ \ y=2 & x=(8,2)\ \ y=3 \\
& x= (9, 5)\ \ y=3\end{matrix}
$$

- Рассчитаем вероятности для классов правого и левого поддерева:

$$
\begin{matrix}
X_l & X_r \\
p_{1}=\frac{2}{3} \approx 0.67\ \ & p_{1}=\frac{0}{4}=0 \\
p_{2}=\frac{1}{3} \approx  0.33\ \ & p_{2}=\frac{2}{4}=0.5 \\
p_{3}=\frac{0}{3}=0\ \ & p_{3}=\frac{2}{4}=0.5 \end{matrix}
$$

- Рассчитаем критерий Джини:

$$
H(X_r) = 0.67 \cdot (1-0.67) + 0.33 \cdot (1-0.33) + 0 \cdot (1-0) \approx  0.44
$$

$$
H(X_l) = 0 \cdot (1-0) + 0.5 \cdot (1-0.5) + 0.5 \cdot (1-0.5) = 0.5
$$

- Расчищаем критерий ошибки:

$$
Q_1(X_m, j, t) = \frac{|X_l|}{|X_m|} \cdot 0.44 + \frac{|X_r|}{|X_m|} \cdot 0.5 \approx \frac{3}{7} \cdot 0.44 + \frac{4}{7} \cdot 0.5 \approx 0.47
$$

Построим правое и левое поддерево для критерия $[2$-й признак $< 3]$:

$$
\begin{matrix}
X_l & X_r \\
x=(2,2)\ \ y=1 & x=(10,4)\ \ y=2 \\
x=(9,0)\ \ y=2 & x=(3,4)\ \ y=2 \\
x=(2,1)\ \ y=1 & x=(9,5)\ \ y=3 \\
x= (8, 2)\ \ y=3 &  \end{matrix}
$$

- Рассчитаем вероятности для классов правого и левого поддерева:

$$
\begin{matrix}
X_l & X_r \\
p_{1}=\frac{2}{4} = 0.5\ \ & p_{1}=\frac{0}{3}=0 \\
p_{2}=\frac{1}{4} = 0.25\ \ & p_{2}=\frac{2}{3} \approx 0.67 \\
p_{3}=\frac{1}{4} = 0.25\ \ & p_{3}=\frac{1}{3} \approx 0.33 \end{matrix}
$$

- Рассчитаем критерий Джини:

$$
H(X_r) = 0.5 \cdot (1-0.5) + 0.25 \cdot (1-0.25) + 0.25 \cdot (1-0.25) = 0.625
$$

$$
H(X_l) = 0 \cdot (1-0) + 0.67 \cdot (1-0.67) + 0.33 \cdot (1-0.33) \approx 0.442
$$

- Расчищаем критерий ошибки:

$$
Q_2(X_m, j, t) = \frac{|X_l|}{|X_m|} \cdot 0.625 + \frac{|X_r|}{|X_m|} \cdot 0.442 \approx \frac{4}{7} \cdot 0.625 + \frac{3}{7} \cdot 0.442 \approx 0.55
$$

Вывод, так как ошибка должна быть наименьшей $Q_1 \approx 0.47 < Q_2 \approx 0.55$, то выбираем первый критерий $[1$-й признак $< 5]$.
