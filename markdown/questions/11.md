# 11. Градиентный спуск в случае линейной регрессии: развернутые формулы для парной регрессии; матричная запись для множественной регрессии. Проблема выбора длины шага, варианты решения. [[⇧]](../questions-list.md)

## Парная регрессия

В случае парной регрессии имеется только один признак, **модель** имеет вид:

$$
a(x) = w_0 + w_1x
$$

$x$ – значение признака.

**Среднеквадратическая ошибка** имеет вид:

$$
Q(w_0, w_1, X) = \frac{1}{n}∑_{i=1}^{n}(w_0+w_1x_i - y_i)^2
$$

$n$ – объем выборки.

Для нахождения **градиента** требуются частные производные функции $Q$ по переменным $w_0$ и $w_1$:

$$
\frac{\partial Q}{\partial w_0} = \frac{2}{n}∑_{i=1}^{n}(w_0+w_1x_i - y_i)
$$

$$
\frac{\partial Q}{\partial w_1} = \frac{2}{n}∑_{i=1}^{n}(w_0+w_1x_i - y_i) \cdot x_i
$$

Тогда на **итерации** с номером $t$:

$$
w_0^t = w_0^{t-1} - \eta_t \cdot \frac{2}{n}∑_{i=1}^{n}(w_0^{t-1}+w_1^{t-1}x_i - y_i)
$$

$$
w_1^t = w_1^{t-1} - \eta_t \cdot \frac{2}{n}∑_{i=1}^{n}(w_0^{t-1}+w_1^{t-1}x_i - y_i) \cdot x_i
$$

## Множественная регрессия

В общем случае имеется $d$ признаков, **модель** имеет вид:

$$
a(x) = w_0x^0 + w_1x^1 + \ldots + w_dx^d, \quad x^0 = 1
$$

$x^j$ – значение $j$-го признака

**Среднеквадратическая ошибка**:

$$
Q(w,X) = \frac{1}{n}∑_{i=1}^{n}(\langle w, x_i \rangle - y_i)^2 = \frac{1}{n} || X \cdot w-y ||^2
$$

$n$ – объем выборки.

$\langle w, x_i \rangle$ – скалярное произведение вектора весов $w$ и вектора признаков $i$-го объекта.

**Градиент функции**:

$$
\nabla Q(w,X) = \begin{pmatrix} \frac{\partial Q}{\partial w_0}\\
\frac{\partial Q}{\partial w_1}\\
\ldots \\
\frac{\partial Q}{\partial w_d}\\
\end{pmatrix} = \frac{2}{n}X^T \cdot ( X \cdot w - y)
$$

**Итерация** с номером $t$:

$$
w^t = w^{t-1} - \eta_t \cdot \frac{2}{n}X^T \cdot ( X \cdot w^{t-1} - y)
$$

## Проблема выбора длины шага

Формальных правил не существует, но есть некоторые закономерности:

- При **маленькой** длине шага алгоритм будет медленно, но верно приближаться к точке минимума (большое количество итераций).
- При **большой** длине шага появляется риск «перепрыгивания» через точку минимума, есть даже риск, что последовательность не сойдется.

Можно использовать **переменную длину шага**:

- На **первых** итерациях, когда до «минимума» еще далеко, использовать относительно **большой шаг**.
- Через некоторое количество итераций уменьшать длину шагов:

  $$\eta_t = \frac{k}{t}$$

  $k$ – константа, побирается эвристически.
