# 22. Деревья решений: принцип разбиения вершин, определение прогноза в листе. Критерии останова. Результаты построения, вид полученного алгоритма. [[⇧]](../questions-list.md)

## Принцип разбиения вершин

- Выбирается **корень**, который разбивает выборку на две.
- Каждый из потомков корня разбивает выборку на две выборки.

## Предикаты

- Порог на признак: $[x_j < t]$.
- Предикат с линейной моделью: $[\langle w, x \rangle < t]$.
- Предикат с метрикой: $[p(x, x_0) < t]$.

> Даже с простейшим предикатом можно строить сложные модели.

## Поиск разбиения

`Пусть:` в вершину $m$ попало множество $X_m$ объектов из выборки.

`Используем:` порог на признак, параметры в условии $[x_j < t]$ должны быть выбраны так, чтобы **минимизировать критерий ошибки**:

$$Q (X_m ,j,t) - \min_{j, t}$$

$x_j$ – $j$-й признак.

$t$ – значение порога.

### Возможное решение

Параметры $j$ и $t$ находятся перебором. После того как параметры $j$ и $t$ выбраны, множество $X_m$ разбивается на два подмножества:

$$X_l = \lbrace x \in X_m |[x^j < t]\rbrace \quad X_r = \lbrace x \in X_m |[x^j \ge t] \rbrace$$

Каждое подмножество соответствует своей **дочерней вершине**.

Далее процедура повторяется для каждой из дочерних вершин, дерево **углубляется**.

## Определение прогноза в листе

`Пусть:` вершина $m$ не требует разделения (объявлена листом).

`Требуется:` определить оптимальный прогноз для объектов подвыборки $X_m$:

- Для **регрессии**: если $Q(X_m, j, t)$ – среднеквадратичная ошибка, то оптимальный прогноз:
  $$a_m = \frac{1}{|X_m |} \cdot ∑_{i \in X_m} y_i$$
- Для **классификации**: оптимальный прогноз — наиболее популярный класс в подвыборке $X_m$:
  $$a_m = argmax_{y \in Y}∑_{x_i \in X_m} [y_i =y]$$

  - **Вероятность принадлежности** $k$-му классу — доля объектов $k$-го класса в подвыборке $X_m$:
    $$p_{mk} = \frac{1}{|X_m |} \cdot ∑_{x_i \in X_m} [y_i = k]$$

## Результаты построения

- Дерево разбивает признаковое пространство на **области** $R_1, R_2, \ldots, R_J$.
- Каждая область $R_j$ соответствует **листу**.
- В области $R_j$ строится константный **прогноз** $a_j$.

### Алгоритм обучения

$$a (X) =∑_{j=1}^{J} a_j \cdot [x∈R_J ]$$

## Критерий останова

- Ограничение глубины дерева.
- Ограничение количества листьев.
- Задание минимального числа объектов в вершине.
- Задание минимального уменьшения разброса ответов при разбиении.
- Принадлежность всех объектов в вершине одному классу.
- Ограничение на количество объектов в вершине.
