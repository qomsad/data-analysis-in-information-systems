# 20. Прогнозирование вероятности принадлежности классу. Обобщенные линейные модели. Модель логистической регрессии: логит-преобразование; обучение модели. Кросс-энтропия. [[⇧]](../questions-list.md)

## Прогнозирование вероятности принадлежности классу

Рассматривается задача **бинарной классификации**.

Ранее были рассмотрены линейные модели классификации, которые позволяли прогнозировать **метки классов** по признаковому описанию объектов.

В задачах бинарной классификации часто требуется не только предсказать метки классов, но и получить оценки $b(x)$ — **вероятности** принадлежности объектов каждому классу.

**Определим** модель $b(x)$ предсказывает вероятность принадлежности объекта $x$ классу $+1$, если среди объектов с $b(x)=p$ доля принадлежащих этому классу равна $p$.

Обозначим **вероятность** $\pi(x)$, которую требуется предсказать:

$$
\pi(x) = P(y=1 | x)
$$

$P(y=1 | x)$ – условная вероятность принадлежности объекта $x$ к классу $1$.

## Обобщенные линейные модели

**Идея**: для получения прогноза использовать модель линейной регрессии:

$$
\pi(x) \approx \langle w, x \rangle
$$

Соображение в пользу этой идеи:

$$
\pi(x) = 1 \cdot P(y=1|x) + 0 \cdot P(y=0|x) = M(y|x)
$$

$M(y|x)$ — условное математическое ожидание $y$.

Однако значение $\langle w_k, x \rangle$ не обязательно принадлежит отрезку $[0, 1]$.

> Будут прогнозироваться значения, которые нельзя интерпретировать как вероятности.

Возможное решение проблемы — использовать **обобщенные линейные модели**:

- Подобрать функцию $g: [0, 1] \rightarrow \mathbb{R}$, которая переводит интервал $[0, 1]$ на множество всех действительных чисел.
- Тогда обратная функция $g^{-1}: \mathbb{R} \rightarrow [0, 1]$.
- Рассматриваем задачу линейной регрессии:
  $$M(y|x) \approx g^{-1}(\langle w, x \rangle)$$
  > Оценка строится не для $M(y|x)$, а для $g^{-1}(\langle w, x \rangle)$

В случае бинарной классификации удобно использовать **сигмоиду**:

$$
\pi(x) = \sigma(\langle w_k, x \rangle) = \frac{e^{\langle w_k, x \rangle}}{1+ e^{-\langle w_k, x \rangle}}
$$

Для одномерного случая (один признак):

$$
\langle w, x \rangle = w_0 + w_1x
$$

$w_0$ – определяет положение центра сигмоиды.
$w_1$ – определяет форму сигмоиды.

### Логит-преобразование

Задача $\pi(x) = \sigma(\langle w_k, x \rangle)$ эквивалентна задаче:

$$
\ln\frac{\pi(x)}{1 - \pi(x)} \approx \langle w, x \rangle
$$

> Логит приближается линейной комбинацией значений признаков.

## Логистическая регрессия

Для обучения модели используется **метод максимального правдоподобия**:

$$
L(x) = \prod_{i=1}^{n} \pi(x_i)^{y_i} \cdot (1 - \pi(x_i))^{1-y_i}
$$

После логарифмирования:

$$
\ln L(x) = y_i \ln \pi(x_i) \cdot (1-y_i) \ln (1 - \pi(x_i)^{1-y_i}
$$

## Кросс-энтропия

Чтобы свести задачу к задаче минимизации, возьмем отрицание функции, таким образом **функция потерь** имеет вид:

$$- \ln L(x) = - \sum_{i=1}^{n}(y_i \ln \pi(x_i) \cdot (1-y_i) \ln (1 - \pi(x_i)^{1-y_i}))$$

$- \ln L(x)$ – такая функция потерь имеет название $log\text{-}loss$, **кросс-энтропия**.

Задача минимизации этой функции хорошо решается численно (градиентными методами), так как функция имеет единственный глобальный минимум.

> В случае линейно-разделимых классов может возникнуть проблема переобучения, возможное решение использование регуляризаторов.

## Прогнозирование откликов

Вероятности, которые вычислены с помощью логистической регрессии, можно использовать для решения задачи классификации — прогнозирование меток классов.

Для **прогнозирования** нужно:

- Задать значения **порога** для значений вероятности $p_0$.
- Использовать уже ранее рассмотренный алгоритм:
  $$a(x) = [\pi(x) > p_0]$$

`Пример:` Была обучена модель логистической регрессии с двумя входными признаками.

Парамеры модели: $w = (-2, 1), w_0 =2$

Необходимо:

1. Записать правило прогнозирования метки класса объекта $x$ при значении порога отсечения равным $p_0$.
2. Дана тестовая выборка $X$:

$$
X = \begin{pmatrix}4 \quad 0\\
1 \quad 3 \\
3 \quad 3\\
1 \quad 2 \end{pmatrix}
$$

Найти предсказания модели логистической регрессии для $X$ при заданных значениях порога $p_1=0.2$

Решение:

Правило прогнозирования метки класса:

$$
a(x) = [\pi(x) > p_0]
$$

Рассчитаем прогноз модели:

$$
y = w \cdot X = \begin{pmatrix}1 \quad 4 \quad 0\\
1 \quad 1 \quad 3 \\
1 \quad 3 \quad 3\\
1 \quad 1 \quad 2 \end{pmatrix} \cdot \begin{pmatrix} 2 \\
-2 \\
1\end{pmatrix} = \begin{pmatrix} -6 \\
3 \\
-1\\
2 \end{pmatrix}
$$

Рассчитаем метки классов для $p_1=0,2$:

$k = \lbrace{0, 1 \rbrace}$

$$
a_k(x) = \begin{pmatrix} [\pi(-6) > 0.2] \\
[\pi(3) > 0.2] \\
[\pi(-1) > 0.2]\\
[\pi(2) > 0.2] \end{pmatrix} = \begin{pmatrix} [\frac{1}{1+ e^{6}} > 0,2] \\
[\frac{1}{1+ e^{-3}} > 0,2] \\
[\frac{1}{1+ e^{1}} > 0,2]\\
[\frac{1}{1+ e^{-2}} > 0,2] \end{pmatrix} = \begin{pmatrix} [0.002> 0,2] \\
[0.952 > 0,2] \\
[0.269 > 0,2]\\
[0.880 > 0,2] \end{pmatrix} = \begin{pmatrix} 0 \\
1 \\
1\\
1 \end{pmatrix}
$$
