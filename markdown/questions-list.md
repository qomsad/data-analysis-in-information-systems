# Анализ данных в информационных системах

## Вопросы

### [1. Концепция разведочного анализа данных. Цели разведочного анализа. Основные методы разведочного анализа.](questions/1.md)

### [2. Статистическая зависимость случайных величин. Корреляционная зависимость. Выборочный коэффициент корреляции Пирсона, его свойства.](questions/2.md)

### [3. Значимость выборочного коэффициента корреляции. Проверка гипотезы о значимости выборочного коэффициента корреляции Пирсона: постановка задачи, используемый критерий, алгоритм проверки нулевой гипотезы.](questions/3.md)

### [4. Выборочные коэффициенты ранговой корреляции Спирмена и Кендалла, их свойства. Проверка гипотез о значимости выборочных коэффициентов корреляции Спирмена и Кендалла.](questions/4.md)

### [5. Исследование взаимосвязей категориальных признаков: таблицы сопряженности; применение критерия хи-квадрат для анализа зависимости категориальных признаков; коэффициент Крамера, его свойства.](questions/5.md)

### [6. Технологии бизнес-анализа. Общая схема Knowledge Discovery in Databases, краткая характеристика основных этапов. Data Mining: варианты определений, основные особенности методов.](questions/6.md)

### [7. Понятие о машинном обучении. Общая постановка задачи обучения. Обучение с учителем, без учителя, частичное обучение. Обучение с подкреплением. Применимость методов машинного обучения.](questions/7.md)

### [8. Машинное обучение с учителем: признаковое описание объекта; обучающая выборка; модель обучения; функционал ошибки; функция потерь. Формальная постановка задачи обучения.](questions/8.md)

### [9. Общее описание модели линейной регрессии: семейство алгоритмов, его геометрическая интерпретация; функционалы ошибки, обучение модели (в развернутой и в матричной форме).](questions/9.md)

### [10. Возможность аналитического решения задачи обучения регрессии; недостатки этого подхода. Оптимизационный подход. Градиент функции, его свойства. Алгоритм градиентного спуска (общий случай).](questions/10.md)

### [11. Градиентный спуск в случае линейной регрессии: развернутые формулы для парной регрессии; матричная запись для множественной регрессии. Проблема выбора длины шага, варианты решения.](questions/11.md)

### [12. Вычислительные сложности градиентного спуска. Алгоритм стохастического градиентного спуска (SGD). Особенности сходимости градиентного спуска. Преимущества метода SGD.](questions/12.md)

### [13. Недообучение и переобучение. Основные подходы к выявлению переобучения. Признак переобученности линейной модели. Мультиколлинеарность признаков, ее связь с возможностью переобучения модели.](questions/13.md)

### [14. Регуляризация линейной модели машинного обучения (основная идея). Типы регуляризаторов, особенности каждого типа.](questions/14.md)

### [15. Метрики качества модели регрессии: MAE, MSE, коэффициент детерминации, квантильная ошибка. Вероятностный смысл метрик.](questions/15.md)

### [16. Общее описание модели бинарной линейной классификации: семейство алгоритмов; геометрическая интерпретация; пороговая функция потерь, ее основной недостаток; верхние оценки пороговой функции.](questions/16.md)

### [17. Метрики качества алгоритмов в задачах бинарной классификации. Типы ошибок, матрица ошибок. Учет возможного дисбаланса классов и разной цены ошибок.](questions/17.md)

### [18. Оценка принадлежности классу в задачах бинарной классификации. Метрики качества оценок принадлежности классу.](questions/18.md)

### [19. Многоклассовая классификация: постановка задачи, подход ONE-VS-ALL, метрики качества алгоритмов. Методы работы с несбалансированными выборками.](questions/19.md)

### [20. Прогнозирование вероятности принадлежности классу. Обобщенные линейные модели. Модель логистической регрессии: логит-преобразование; обучение модели. Кросс-энтропия.](questions/20.md)

### [21. Общая характеристика решающих деревьев: структура, вид разделяющей поверхности в задаче классификации, вид восстановленной зависимости в задаче регрессии, возможность переобучения.](questions/21.md)

### [22. Деревья решений: принцип разбиения вершин, определение прогноза в листе. Критерии останова. Результаты построения, вид полученного алгоритма.](questions/22.md)

### [23. Критерий ошибки в решающих деревьях. Критерии информативности в задачах регрессии и классификации.](questions/23.md)

### [24. Использование категориальных признаков в решающих деревьях: возможных подходы. Построение бинарных деревьев с разбиением множества значений.](questions/24.md)

### [25. Основная идея построения композиции алгоритмов: последовательность действий, формирование итогового ответа. Рандомизация: назначение, основные подходы.](questions/25.md)

### [26. Разложение ошибки алгоритма машинного обучения. Смещение и разброс композиции алгоритмов; подходы к уменьшению корреляции базовых алгоритмов.](questions/26.md)

### [27. Рандомизация построения решающих деревьев. Алгоритм построения случайного леса. Особенности модели «случайный лес».](questions/27.md)

### [28. Общая характеристика задач машинного обучения без учителя. Основные типы задач обучения без учителя.](questions/28.md)

### [29. Постановка задачи кластеризации объектов. Неоднозначность решения задачи кластеризации. Области применения. Типы кластерных структур.](questions/29.md)

### [30. Функционалы качества кластеризации: два основных подхода. Применение силуэта для оценки качества кластеризации. Основные метрики качества кластеризации.](questions/30.md)

### [31. Метод k-средних: основной алгоритм; вариации метода. Особенности метода. Подбор оптимального числа кластеров с помощью Inertia.](questions/31.md)

### [32. EM-алгоритм: базовые предположения; описание алгоритма. Особенности алгоритма.](questions/32.md)

### [33. Методы кластеризации, основанные на плотности точек: основная идея. Алгоритм DBSCAN, его достоинства и недостатки. Рекомендации по подбору параметров.](questions/33.md)
